{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 盘一盘 Python 系列 4 - Pandas (下)\n",
    "\n",
    "\n",
    ">*by* 马川整理  *燕山大学*\n",
    "\n",
    ">原创： 王圣元 王的机器 2019-04-17\n",
    "\n",
    ">**本人将王圣元的公众号([王的机器](https://mp.weixin.qq.com/s/Fo-UIGnsoU2nBVLxSw_nVw))文章转录为ipynb文档，并据本课程所需有所增删修改**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着上篇继续后面三个章节\n",
    "\n",
    "**提纲**\n",
    "\n",
    "![](images/Pandas提纲2.png)\n",
    "\n",
    "## 4 数据表的合并和连接\n",
    "\n",
    "数据表可以按<font color=\"red\"><b>「键」</b></font>合并，用 merge 函数；可以按<font color=\"red\"><b>「轴」</b></font>来连接，用 concat 函数。\n",
    "\n",
    "### 4.1 合并\n",
    "\n",
    "合并用 merge 函数，语法如下：\n",
    "\n",
    ">pd.merge( df1, df2, how=s, on=c )\n",
    "\n",
    "c 是 df1 和 df2 共有的一栏，合并方式 (how=s) 有四种：\n",
    "\n",
    "1. 左连接 (left join)：合并之后显示 df1 的所有行\n",
    "\n",
    "2. 右连接 (right join)：合并之后显示 df2 的所有行\n",
    "\n",
    "3. 外连接 (outer join)：合并所有行\n",
    "\n",
    "4. 内连接 (inner join)：合并df1 和 df2 共有的所有行 (默认情况)\n",
    "\n",
    "首先创建两个 DataFrame：\n",
    "\n",
    "* df_math：四位同学的数学成绩\n",
    "\n",
    "* df_computer：五位同学的计算机成绩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_math = pd.DataFrame( {'Name': ['张三丰','乔峰','小龙女','令狐冲'],\n",
    "                          'Math': [94.4, 75.0, 88.2, 85.6]})\n",
    "df_math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_computer = pd.DataFrame( {'Name': ['乔峰','小龙女','令狐冲','张无忌','杨过'],\n",
    "                           'Computer' : [95.6, 76.5, 92.3, 98.0, 81.5]})\n",
    "df_computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来用 df_math  和 df_computer 展示四种合并。\n",
    "\n",
    "<font color=\"red\"><b>left join</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge( df_math, df_computer, how='left' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按 **df_math** 里 Name 栏里的值来合并数据\n",
    "\n",
    "* df_computer 里 Name 栏里没有 张三丰，因此 Computer 为 NaN\n",
    "\n",
    "* df_computer 里 Name 栏里的 张无忌 和 杨过 不在 df_math 里 Name 栏，因此丢弃\n",
    "\n",
    "<font color=\"red\"><b>right join</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge( df_math, df_computer, how='right' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按 **df_computer** 里 Name 栏里的值来合并数据\n",
    "\n",
    "* df_math 里 Name 栏里没有 张无忌 和 杨过，因此 Math 为 NaN\n",
    "\n",
    "* df_math 里 Name 栏里的 张三丰 不在 df_computer 里 Name 栏，因此丢弃\n",
    "\n",
    "<font color=\"red\"><b>outer join</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge( df_math, df_computer, how='outer' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按 df_math 和 df_computer 里 Name 栏里的**所有值**来合并数据\n",
    "\n",
    "* df_math 里 Name 栏里没有 张无忌 和 杨过，因此 Math 为 NaN\n",
    "\n",
    "* df_computer 里 Name 栏里没有 张三丰，因此 Computer 为 NaN\n",
    "\n",
    "<font color=\"red\"><b>inner join</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge( df_math, df_computer, how='inner' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按 df_math 和 df_computer 里 Name 栏里的**共有值**来合并数据\n",
    "\n",
    "* df_math 里 Name 栏里的 张三丰 不在 df_computer 里 Name 栏，因此丢弃\n",
    "\n",
    "* df_computer 里 Name 栏里的 张无忌 和 杨过 不在 df_math 里 Name 栏，因此丢弃\n",
    "\n",
    "\n",
    "<font color=\"red\"><b>注意：</b></font>\n",
    "\n",
    "在使用merge()函数进行合并时，默认会使用重叠的列索引做为合并键，并采用内连接方式合并数据，即取行索引重叠的部分。\n",
    "\n",
    "比如， df_math 和 df_computer 中都有 Name 列，所以默认按 Name 列合并。\n",
    "\n",
    "如果两个DataFrame有多个重叠的列(即多个列索引相同，比如两个DataFrame中都有ID列和Name列)，可以使用**参数on**指定哪个列作为合并键(比如，on='ID'则是指定ID列作为合并键)，自己试试吧。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 连接\n",
    "\n",
    "Numpy 数组可相互连接，用 np.concat；同理，Series 和 DataFrame 也可相互连接，用 pd.concat。\n",
    "\n",
    "#### 连接 DataFrame\n",
    "\n",
    "<font color=\"red\"><b>沿着行连接 (axis = 0)</b></font>\n",
    "\n",
    "先创建两个 DataFrame，df1 和 df2。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df1 = pd.DataFrame( np.arange(12).reshape(3,4), \n",
    "                    columns=['东邪','西毒','南帝','北丐'])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame( np.arange(6).reshape(2,3),\n",
    "                    columns=['西毒','北丐','东邪'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "沿着行连接分两步\n",
    "\n",
    "1. 先把 df1 和 df2 列标签补齐\n",
    "\n",
    "2. 再把 df1 和 df2 纵向连起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat( [df1, df2] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到的 DataFrame 的 index = [0,1,2,0,1]，有重复值。如果 index 不包含重要信息 (如上例)，可以将 ignore_index 设置为 True，这样就得到默认的 index 值了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat( [df1, df2], ignore_index=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><b>沿着列连接 (axis = 1)</b></font>\n",
    "\n",
    "先创建两个 DataFrame，df1 和 df2。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame( np.arange(6).reshape(3,2), \n",
    "                    index=['胜','负','平'],\n",
    "                    columns=['东邪','西毒'] )\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame( 5 + np.arange(4).reshape(2,2), \n",
    "                    index=['胜','平'], \n",
    "                    columns=['南帝','北丐'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "沿着列连接分两步\n",
    "\n",
    "1. 先把 df1 和 df2 行标签补齐\n",
    "\n",
    "2. 再把 df1 和 df2 横向连起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat( [df1, df2], axis=1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 数据表的重塑和透视\n",
    "\n",
    "有许多用于重新排列表格型数据的基础运算。这些函数也称作重塑（reshape）或轴向旋转（pivot）运算。\n",
    "\n",
    "重塑 (reshape) 和透视 (pivot) 两个操作只改变数据表的布局 (layout)：\n",
    "\n",
    "* 重塑用 stack 和 unstack 函数 (互为逆转操作)\n",
    "\n",
    "* 透视用 pivot 和 melt 函数 (互为逆转操作)\n",
    "\n",
    "\n",
    "\n",
    "### 5.1 重塑\n",
    "\n",
    "重塑就是通过改变数据表里面的「行索引」和「列索引」来改变展示形式，从本质上说，就是重塑层次化索引(多层索引)。\n",
    "\n",
    "**行列旋转**\n",
    "\n",
    "* 列索引 → 行索引，用 stack 函数\n",
    "\n",
    "* 行索引 → 列索引，用 unstack 函数\n",
    "\n",
    "#### 单层 DataFrame\n",
    "\n",
    "创建 DataFrame df (1 层行索引，1 层列索引)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = ['JD', 'AAPL']\n",
    "data = {'行业': ['电商', '科技'],\n",
    "        '价格': [25.95, 172.97],\n",
    "        '交易量': [27113291, 18913154]}\n",
    "df = pd.DataFrame( data, index=symbol )\n",
    "df.columns.name = '特征'\n",
    "df.index.name = '代号'\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上表中可知：\n",
    "\n",
    "* 行索引 = [JD, AAPL]，名称是代号\n",
    "\n",
    "* 列索引 = [行业, 价格, 交易量]，名称是特征\n",
    "\n",
    "<font color=\"red\"><b>stack: 列索引 → 行索引</b></font>\n",
    "\n",
    "列索引 (特征) 变成了行索引，原来的 DataFrame df 变成了两层 Series (第一层索引是代号，第二层索引是特征)。\n",
    "\n",
    "![](images/stack.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2i_Series = df.stack()\n",
    "c2i_Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><b>unstack: 行索引 → 列索引</b></font>\n",
    "\n",
    "行索引 (代号) 变成了列索引，原来的 DataFrame df 也变成了两层 Series (第一层索引是特征，第二层索引是代号)。\n",
    "\n",
    "![](images/unstack.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2c_Series = df.unstack()\n",
    "i2c_Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 透视\n",
    "\n",
    "多个**时间序列数据**(在多个时间点观察或测量到的数据)通常是以所谓的**“长格式”（long）或“堆叠格式”（stacked）**存储在数据库和CSV中的。\n",
    "\n",
    "因此，经常有重复值出现在各列下，因而导致源表不能传递有价值的信息。这时可用「透视」方法调整源表的布局用作更清晰的展示。\n",
    "\n",
    "在 Pandas 里透视的方法有两种：\n",
    "\n",
    "* 用 pivot_table 函数将「长格式」**旋转**为「宽格式」，\n",
    "\n",
    "* 用 melt 函数将「宽格式」**旋转**为「长格式」，\n",
    "\n",
    "本节使用的数据描述如下：\n",
    "\n",
    "* 5 只股票：AAPL, JD, BABA, FB, GS\n",
    "\n",
    "* 4 个交易日：从 2019-02-21 到 2019-02-26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/Stock.csv', parse_dates=[0], dayfirst=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上表看出有 20 行 (5 × 4) 和 8 列，在 Date 和 Symbol 那两列下就有重复值，4 个日期和 5 个股票在 20 行中分别出现了 5 次和 4 次。\n",
    "\n",
    "这就是多个时间序列（或者其它带有两个或多个键的可观察数据，这里，我们的键是Date和Symbol）的长格式。表中的每行代表一次观察。\n",
    "\n",
    "关系型数据库（如MySQL）中的数据经常都是这样存储的，因为固定架构（即列名和数据类型）有一个好处：随着表中数据的添加，Symbol列中的值的种类能够增加。在前面的例子中，Date和Symbol通常就是主键（关系型数据库中的术语，是表中的一个或多个字段，它的值用于唯一地标识表中的某一条记录），不仅提供了关系完整性，而且提供了更为简单的查询支持。有的情况下，使用这样的数据会很麻烦，你可能会更喜欢不同的Symbol值分别形成一列，Date列中的时间戳则用作索引。DataFrame的pivot_table方法完全可以实现这个转换：\n",
    "\n",
    "#### 从长到宽 (pivot_table)\n",
    "\n",
    "**pandas.pivot_table的重点在于reshape, 通俗理解就是合并同类项，默认计算相同数据的均值并返回。**\n",
    "\n",
    "当我们做数据分析时，只关注不同股票在不同日期下的 Adj Close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[:,[0,1,6]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么可用 pivot 函数将原始 data「透视」成一个新的 DataFrame，起名 close_price。在 pivot 函数中\n",
    "\n",
    "* 将 index 设置成 ‘Date’\n",
    "\n",
    "* 将 columns 设置成 ‘Symbol’\n",
    "\n",
    "* 将 values 设置 ‘Adj Close’\n",
    "\n",
    "close_price 实际上把 data[‘Date’] 和 data[‘Symbol’] 的**唯一值**当成支点(pivot 就是支点的意思) 创建一个 DataFrame，其中\n",
    "\n",
    "* 行标签 = 2019-02-21, 2019-02-22, 2019-02-25, 2019-02-26\n",
    "\n",
    "* 列标签 = AAPL, JD, BABA, FB, GS\n",
    "\n",
    "在把 data[‘Adj Close’] 的值放在以如上的行标签和列标签创建的 close_price 来展示。\n",
    "\n",
    "代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_price = data.pivot_table( index='Date',\n",
    "                          columns='Symbol',\n",
    "                          values='Adj Close' )\n",
    "close_price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果觉得 Adj Close 不够，还想加个 Volume 看看，这时支点还是 data[‘Date’] 和 data[‘Symbol’]，但是要透视的值增加到 data[['Adj Close', 'Volume']] 了。pivot_table 函数返回的是两个透视表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.pivot_table( index='Date',\n",
    "            columns='Symbol',\n",
    "            values=['Adj Close','Volume'] )\n",
    "\n",
    "# 用下面写法亦可\n",
    "# data.pivot_table( index='Date',\n",
    "#            columns='Symbol')[['Adj Close','Volume']] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果不设置 values 参数，那么 pivot_table 函数返回的是六个透视表。(源表 data 有八列，两列当了支点，剩下六列用来透视)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_pivot = data.pivot_table( index='Date', \n",
    "                        columns='Symbol' )\n",
    "all_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 从宽到长 (melt)\n",
    "\n",
    "旋转DataFrame的逆运算是pandas.melt，它合并多个列成为一个，产生一个比输入长的DataFrame。\n",
    "\n",
    "当使用pandas.melt，我们必须指明哪些列是分组指标。具体来说，函数 melt 实际是将「源表」转化成 id-variable 类型的 DataFrame，下例将\n",
    "\n",
    "* Date 和 Symbol 列当成 id，即分组指标\n",
    "\n",
    "* 其他列 Open, High, Low, Close, Adj Close 和 Volume 当成 variable，而它们对应的值当成 value\n",
    "\n",
    "代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_data = pd.melt( data, id_vars=['Date','Symbol'] )\n",
    "melted_data.head(5).append(melted_data.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "新生成的 DataFrame 有 120 行 (4 × 5 × 6)\n",
    "\n",
    "* 4 = data['Date'] 有 4 个日期\n",
    "\n",
    "* 5 = data['Symbol'] 有 5 只股票\n",
    "\n",
    "* 6 = Open, High, Low, Close, Adj Close 和 Volume 这 6 个变量\n",
    "\n",
    "在新表 melted_data 中\n",
    "\n",
    "* 在参数 id_vars 设置的 Date 和 Symbol 还保持为 columns\n",
    "\n",
    "* 此外还多出两个 columns，一个叫 variable，一个叫 value\n",
    "\n",
    "  * variable 列下的值为 Open, High, Low, Close, Adj Close 和 Volume\n",
    "\n",
    "  * value 列下的值为前者在「源表 data」中的值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 数据表的分组和聚合\n",
    "\n",
    "DataFrame 中的数据可以根据某些规则分组，然后在每组的数据上计算出不同统计量。这种操作称之为 split-apply-combine（拆分－应用－合并）。\n",
    "\n",
    "![](images/7-2.png)\n",
    "\n",
    "第一个阶段，pandas对象（无论是Series、DataFrame还是其他的）中的数据会根据你所提供的一个或多个键被拆分（split）为多组。拆分操作是在对象的特定轴上执行的。例如，DataFrame可以在其行（axis=0）或列（axis=1）上进行分组。然后，将一个函数应用（apply）到各个分组并产生一个新值。最后，所有这些函数的执行结果会被合并（combine）到最终的结果对象中。结果对象的形式一般取决于数据上所执行的操作。\n",
    "\n",
    "该 split-apply-combine 过程有两步(apply-combine合为一步完成)：\n",
    "\n",
    "**Step1 ：数据分组(split)**\n",
    "\n",
    "* groupby 方法\n",
    "\n",
    "**Step2 ：数据聚合(apply-combine)**\n",
    "\n",
    "* 使用内置函数——sum / mean / max / min / count等\n",
    "\n",
    "* 使用自定义函数—— agg ( aggregate ) 方法\n",
    "\n",
    "* 自定义更丰富的分组运算—— apply 方法\n",
    "\n",
    "agg 方法将一个函数使用在**一个数列**上，然后返回一个**标量**的值。也就是说agg每次传入的是一列数据，对其聚合后返回标量。\n",
    "\n",
    "apply 是一个更一般化的方法，会将当前分组后的数据一起传入，返回多维数据。\n",
    "\n",
    "### 6.1 数据准备\n",
    "\n",
    "本节使用数据：**泰坦尼克数据集**\n",
    "\n",
    "* PassengerId => 乘客编号\n",
    "\n",
    "* Survived => 获救情况（1为获救，0为未获救）\n",
    " \n",
    "* Pclass => 乘客等级(1/2/3等舱位)\n",
    "\n",
    "* Name => 乘客姓名\n",
    "\n",
    "* Sex => 性别\n",
    "\n",
    "* Age => 年龄\n",
    "\n",
    "* SibSp => 堂兄弟/妹个数\n",
    "\n",
    "* Parch => 父母与小孩个数\n",
    "\n",
    "* Ticket => 船票信息\n",
    "\n",
    "* Fare => 票价\n",
    "\n",
    "* Cabin => 客舱\n",
    "\n",
    "* Embarked => 登船港口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv(r'data\\Titanic.csv')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**用前面所学透视一下数据：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "titanic.pivot_table(index='Sex',columns='Pclass',values='Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.pivot_table(index='Sex',columns='Pclass',values='Survived',aggfunc='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.pivot_table(index='Sex',columns='Pclass',aggfunc={'Survived':'sum','Age':'mean'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 分组 (grouping)\n",
    "\n",
    "用某一特定标签 (label) 将数据 (data) 分组的语法如下：\n",
    "\n",
    ">data.groupBy( label )\n",
    "\n",
    "<font color=\"red\"><b>单标签分组</b></font>\n",
    "\n",
    "首先我们按 Symbol 来分组："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = titanic.groupby('Sex')\n",
    "grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "又要提起那句说了无数遍的话「万物皆对象」了。这个 grouped 也不例外，当你对如果使用某个对象感到迷茫时，用 dir() 来查看它的「属性」和「内置方法」。以下几个属性和方法是我们感兴趣的：\n",
    "\n",
    "* ngroups: 组的个数 (int)\n",
    "\n",
    "* size(): 每组元素的个数 (Series)\n",
    "\n",
    "* groups: 每组元素在原 DataFrame 中的索引信息 (dict)\n",
    "\n",
    "* get_groups(label): 标签 label 对应的数据 (DataFrame)\n",
    "\n",
    "下面看看这些属性和方法的产出结果。\n",
    "\n",
    "数据里性别为male和female，因此有2组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.ngroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每组的信息条数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "女士 (female) 的索引 1,   2,   3,   8,   9,  ...，男士( male) 的索引0,   4,   5,   6,   7,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查查 'male' 组里的数据的前五行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.get_group('male').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><b>多标签分组</b></font>\n",
    "\n",
    "groupBy 函数除了支持单标签分组，也支持多标签分组 (将标签放入一个列表中)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped2 = titanic.groupby(['Sex','Pclass'])\n",
    "grouped2.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 聚合 (aggregating)\n",
    "\n",
    "**6.3.1 使用内置函数——sum / mean / max / min / count等**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped.mean()\n",
    "# grouped.sum()\n",
    "# grouped.max()\n",
    "# grouped.min()\n",
    "# grouped.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.3.2 使用自定义函数—— agg ( aggregate ) 方法**\n",
    "\n",
    "agg 方法将一个函数使用在**一个数列**上，然后返回一个**标量**的值。也就是说agg每次传入的是一列数据，对其聚合后返回标量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped['Survived'].agg(np.mean)\n",
    "grouped.agg(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "titanic.groupby(['Sex','Pclass'])['Survived'].agg(['mean','sum'])\n",
    "# 或者这样写\n",
    "# titanic.groupby(['Sex','Pclass'])['Survived'].agg([np.mean,np.sum])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将 np.mean 和 np.std 放进列表中，当成是高阶函数 agg() 的参数。上面代码按性别和乘客等级对获救情况求均值与和。\n",
    "\n",
    "既然 agg() 是高阶函数，参数当然也可以是匿名函数 (lambda 函数)，下面我们定义一个对 grouped2 里面每个标签下求最大值和最小值，再求差。注意 lambda 函数里面的 x 就是 grouped2。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped2.agg( lambda x: np.max(x)-np.min(x) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面代码对每个分组在Age、Fare、Parch、PassengerId、SibSp和Survived上求「最大值」和「最小值」的差。真正有价值的信息在 Age、Parch 等栏，但我们可以借此来验证agg使用自定义函数的用法。\n",
    "\n",
    "**6.3.3 自定义更丰富的分组运算—— apply 方法**\n",
    "\n",
    "apply 是一个更一般化的方法：将一个数据分拆-应用-汇总，会将当前分组后的数据一起传入，返回多维数据。\n",
    "\n",
    "有时候返回的值不一定是一个标量的值，有可能是一个数组或是其他类型。此时，agg无法胜任，就需要使用apply了。\n",
    "\n",
    "在看具体例子之前，我们先定一个 top 函数，返回 DataFrame 某一栏中 n 个最大值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top( df, n=5, column='Parch' ):\n",
    "    return df.sort_values(by=column)[-n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将 top 函数用到最原始的数据 (从 csv 中读取出来的) 上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top( titanic )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的**top函数**中，df 代表我传递给它的DataFrame数据，n代表取它的前n行，在这里，n的默认值是5，也就是说在调用这个函数的时候，如果没有给n赋值，n值等于5。column是排序列，函数会先按column升序排序，然后返回最大的n行。在这个时候，agg的方法就不管用的，要是强行使用，就会出错。\n",
    "\n",
    "来，演示一遍错误！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.groupby('Sex').agg(top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=\"red\">Apply 函数</font>**\n",
    "\n",
    "将 top() 函数 apply 到按 Sex 分的每个组上，按每个 Sex 分组打印出来了Parch 栏下的 5 个最大值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.groupby('Sex').apply(top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面在使用 top() 时，对于 n 和 column 我们都只用的默认值 5 和 'Parch'。如果用自己设定的值 n = 2, column = 'SibSp'，写法如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "titanic.groupby(['Sex','Pclass']).apply(top, n=2, column='SibSp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按每个 Sex 和 Pclass 打印出来了 SibSp 栏下的最大的两个值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 排序(Mc补充)\n",
    "\n",
    "排序分为对**索引排序 sort_index** 和对 **值排序 sort_values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = pd.Series(range(4), index=['d','a','b','c'])\n",
    "print(obj)\n",
    "#索引排序\n",
    "obj.sort_index()\n",
    "#值排序\n",
    "# obj.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.DataFrame(np.arange(8).reshape((2,4)),index=['three','one'], columns=['d','a','b','c'])\n",
    "print(frame)\n",
    "\n",
    "# 索引排序\n",
    "frame.sort_index()\n",
    "# frame.sort_index(axis=1)\n",
    "# 降序\n",
    "# frame.sort_index(axis=1, ascending=False)\n",
    "\n",
    "# 值排序\n",
    "# frame.sort_values(by='a',ascending=False)\n",
    "# frame.sort_values(by=['a','b'],ascending=False)\n",
    "# frame.sort_values(by='one',axis=1,ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 总结\n",
    "\n",
    "【合并数据表】用 merge 函数按数据表的共有列进行左/右/内/外合并。\n",
    "\n",
    "![](images/7-3.png)\n",
    "\n",
    "【连接数据表】用 concat 函数对 Series 和 DataFrame 沿着不同轴连接。\n",
    "\n",
    "【重塑数据表】用 stack 函数将「列索引」变成「行索引」，用 unstack 函数将「行索引」变成「列索引」。它们只是改变数据表的布局和展示方式而已。\n",
    "\n",
    "![](images/7-4.png)\n",
    "\n",
    "![](images/7-5.png)\n",
    "\n",
    "![](images/7-6.png)\n",
    "\n",
    "【透视数据表】用 pivot 函数将「一张长表」变成「多张宽表」，用 melt 函数将「多张宽表」变成「一张长表」。它们只是改变数据表的布局和展示方式而已。\n",
    "\n",
    "![](images/7-7.png)\n",
    "\n",
    "![](images/7-8.png)\n",
    "\n",
    "【分组数据表】用 groupBy 函数按不同「列索引」下的值分组。一个「列索引」或多个「列索引」就可以。\n",
    "\n",
    "【聚合数据表】用 agg 函数对每个组做聚合而计算统计量。\n",
    "\n",
    "【split-apply-combine】用 apply 函数做数据分析时美滋滋。\n",
    "\n",
    "![](images/7-9.png)\n",
    "\n",
    "至此，我们已经打好 Python Basics 的基础，能用 NumPy 做数组计算，能用 Pandas 做数据分析，现在已经搞很多事情了。现在我们唯一欠缺的是如何画图或可视化数据，下帖从最基础的可视化工具 Matplotlib 开始讲。Stay Tuned!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![结束](images/end.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
